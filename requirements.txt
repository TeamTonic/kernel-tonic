# =========================
# NVIDIA/CUDA Requirements
# =========================

# PyTorch, TorchVision, and Torchaudio for CUDA
# Recommended: install via pip (CUDA 11.8+)
# https://pytorch.org/get-started/locally/
# Example:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# =========================
# Core dependencies
# =========================

transformers
torch
datasets
huggingface_hub
optimum

# =========================
# Model Acceleration Libraries
# =========================

# Flash Attention 2 (CUDA version, must be built from source for most GPUs)
# https://github.com/Dao-AILab/flash-attention
# pip install flash-attn --no-build-isolation

# xFormers (CUDA version, available on PyPI for most GPUs)
xformers

# =========================
# vLLM (LLM Inference)
# =========================
# vLLM (CUDA version, available on PyPI)
vllm

# =========================
# Quantization Libraries
# =========================

# AutoGPTQ (CUDA version, available on PyPI)
auto-gptq
bitsandbytes

# =========================
# Transformer Engine (NVIDIA version)
# =========================
# https://github.com/NVIDIA/TransformerEngine
# pip install transformer-engine

# =========================
# Training, Logging, Utilities
# =========================

wandb
tensorboard
pyyaml
hydra-core
omegaconf
pytest-cov
peft
tqdm
requests

# =========================
# Project-specific modules
# =========================
# (Add your own modules here)

# =========================
# Notes
# =========================
# - Only include packages above that are available on PyPI or have a clear CUDA install method.
# - For flash-attn and transformer-engine, you may need to build from source for some GPU architectures.
# - Remove or comment out any package not validated for CUDA/NVIDIA GPUs. 